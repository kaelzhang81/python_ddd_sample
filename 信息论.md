# 信息论处理的是客观世界中的不确定性

不确定性才是客观世界的本质属性。

## 信息论的诞生
1948，香农的《通信的数学理论》---给出对信息这一定性概念的定量分析方法。
“通信的基本问题是在一点精确地或近似地复现在另一点所选取的消息。消息通常有意义，即根据某种体系，消息本身指向或关联着物理上或概念上的特定实体。但消息的语义含义与工程问题无关，重要的是一条消息来自于一个所有可能的消息的集合。”

信息论使用“信息熵”的概念，对单个信源的信息量和通信中传递信息的数量与效率等问题做出了解释，并在世界的不确定性和信息的可测量性之间搭建起一座桥梁。

信息的载体是消息，不同的消息带来的信息即使在直观上也不尽相同。（例如：中国足球夺冠比乒乓球夺冠信息要大得多）以不确定性来度量信息是一种合理的方式。不确定性越大的消息可能性越小，其提供的信息量就越大。

## 信息熵
“熵”源于冯诺依曼---一个系统内在的混乱程度。

如果事件A发生的概率为p(A)，则这个事件的自信息量的定义为：
单个事件的自信息量可以计算包含多个符号的信源的信息熵，如果一个离散信源X包含n个符号，每个符号ai的取值为p（ai），则X的信息熵为

信息熵描述了信源每发送一个符号所提供的平均信息量，是信源总体信息测度的均值。
当信源中每个符号取值概率相等时，信息熵取到最大值log2n，意味着信源的随机程度最高。

# 条件熵和信息增益是分类问题中的重要参数

## 条件熵
条件熵：概率论中的条件概率扩展到信息论。
条件熵描述的是两个信源的相关性。
条件熵的意义在于先按照变量X的取值对变量Y进行一次分类，对每个分出来的类别计算其单独的信息熵，再将每个类的信息熵按照X的分布计算其数学期望。

比如：上课教室随便坐的信息熵较大，加上限制条件，比如男左女右，就会带来不确定性的下降。

## 信息增益

互信息等于Y的信息熵减去已知X时Y的条件熵，即X提供的关于Y的不确定性的消除，也可以看成是X给Y带来的信息增益。互信息用于通信领域，信息增益则在机器学习中使用。
在机器学习中，信息增益常常被用于分类特征的选择。特征X带来对训练集Y分类不确定性的减少程度，也就是特征X对训练集Y的区分度。
信息增益比：信息增益/信息熵

# KL散度用于描述两个不同概率分布之间的差异

Kullback-Leibler散度，简称KL散度。KL散度是描述两个概率分布P和Q之间的差异的一种方法，其定义为：
KL散度是对额外信息量的衡量。用来衡量两个分布之间平均每个字符多用的比特殊，也可以表示两个分布之间的距离。

KL散度的两个重要性质是非负性和非对称性。

非负性：KL散度大于或等于0，等号只在两个分布完全相同时取到。

非对称性：《公式》。即用P（X）去近似Q（X）和用Q（X）去近似P（X）得到的偏差是不同的，因此KL散度并不满足数学意义上对距离的定义。
要让《》最小，需呀让Q（X）在P（X）不等于0的位置同样不等于0，《》最小则是让Q（X）在p(X)等于0的位置同样等于0.

# 最大熵原理是分类问题汇总的常用准则

最大熵原理：确定随机变量统计特性时力图最符合客观情况的一种准则。
对于一个未知的概率分布，最坏的情况就是它以等可能性取到每个可能的取值。这个时候的概率分布是均匀的，也就是随机变量的随机程度最高，对他进行预测也就最困难。
其本质在于推断未知分布时不引入任何多余的约束和假设，因而可以得到最不确定的结果，预测风险也就最小。---“不要把所有鸡蛋放在同一个篮子里”。
最大熵模型：将最大熵原理应用到分类问题。

在分类问题中，首先确定若干特征函数作为分类依据。
每一个特征函数对应一个约束条件（其数学期望估计是个无偏估计量）
分类任务就是在这些约束下，确定一个最好的分类模型。
利用最大熵原理，求解出不确定性最大的条件分布，《公式》

特征函数确定的约束条件可以通过拉格郎日乘子的引入去除其影响，转换为无约束优化问题，模型的解存在且唯一。


